---
title: "QualMiner: Exploring qualitative indicators via text mining methods"
author: "Metin Yazici, BeDataDriven B.V."
date: "`r Sys.Date()`"
output:
  html_document:
    theme: cosmo
    css: style.css
    highlight: pygments
    code_folding: hide
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: true
---

<script src="script.js"></script>

```{r setup, include=FALSE}
## Header file
source(file.path("..", "R", "global-header.R"))
## Functions
source(file.path("..", "R", "analysis-methods.R"))
## Libraries
library(tidyverse)
library(tidytext)
library(scales)
library(dygraphs)
## Pre-set the bw theme for ggplot (for plots don't use any custom theme).
ggplot2::theme_set(ggplot2::theme_bw())
## knitr options
knitr::opts_chunk$set(echo = TRUE, paged.print = FALSE)
```

# Introduction

This document explores qualitative indicators from an *ActivityInfo* database
that is monitoring Ecuador.

The document includes the source code used in the analysis. You can choose to
`show` or `hide` by selecting the option from the top right corner if you do not
desire to see the code chunks.

# Data preparation

The data has been extracted from *ActivityInfo* and pre-processed to make it
ready for the analysis. Check the calls in the `R/` directory to see how the
process went on.

### Data import & preparation

Read the data from the source that has been extracted, cleaned, and transformed.
Select the rows where the field type equals to `NARRATIVE`, which indicates that
is a multi-line text field in the *ActivityInfo*. Select these columns and
analyze them by comparing and contrasting with other fields types associated with
the textual field types.

```{r data-source-preparation}
## read data from disk:
form.table <- jsonlite::fromJSON(file.path("..", TEXT.DATA.PATH))
## order matters for display purposes:
col.names <- c(
  "labelFolder",
  "labelForms",
  "Month",
  "question",
  "response",
  "description",
  "partnerName",
  "canton",
  "province"
)
## if we choose a column name by mistake:
stopifnot(!anyDuplicated(col.names) > 0)
compact <- form.table %>% 
  as_tibble() %>% 
  filter(!is.na(response) & type == "NARRATIVE") %>% 
  select(col.names)
kable_truncate(head(compact), c("question", "response", "description"), trunc.level = 10)
```

We do shorten and therefore recode form topics because they seem to be too long
and disarray the plots. The **recode table** is placed below to look up form
labels and their abbreviations:

#### Recode table
```{r recode-table}
## only recode some long label forms, shorter names can stay as it is (e.g.
## `Salud`)
compact <- compact %>% 
  mutate(
    labelFormsRecode = recode(
      labelForms,
      "Alojamiento Temporal" = "Alojamiento",
      "Necesidades básicas/Otro" = "Necesidades",
      "Manejo de la información y entrega directa de la información a la población" = "Población",
      "Manejo de la información para socios y análisis de las necesidades" = "Socios",
      "Protección_VBG" = "VBG",
      "Trata_y_tráfico" = "Tráfico",
      "Acceso_a_educación" = "Educación",
      "Acceso a vivienda y hábitat dignos en comunidades receptoras" = "Hábitat",
      "Medios de vida y formación técnico-profesional" = "Técnico",
      "Cohesión_social" = "SocialCohesión",
      "Apoyo Educacional a Comunidades Receptoras" = "Educacional",
      "Asistencia técnica para VBG-SSR" = "VBG_SSR",
      "Asistencia técnica para protección/gestión de fronteras" = "Fronteras",
      "Asistencia técnica para gestion de la informacion y coordinacion" = "Coordinacion",
      "Asistencia técnica para el sector laboral" = "SectorLaboral",
      "Asistencia técnica para protección" = "Protección",
      "Asistencia técnica para protección de la infancia" = "ProtecciónInfancia",
      "Protección_LGBTI" = "LGBTI"
    )
  )

recode_tbl <- compact %>% select(labelFormsRecode, labelForms) %>% distinct()
knitr::kable(recode_tbl)
```

# Analysis

Before we start, here's some descriptive tables about the nature of data.

Number of partners
```{r}
compact %>% count(partnerName, sort = TRUE) %>% knitr::kable()
```

Number of Cantons and Provinces

```{r}
compact %>% count(province, canton, sort = TRUE) %>% knitr::kable()
```

## Response quality

Response quality means how much response the questions receive. The idea is to
find relations that affect the response quality to understand if they work or
not under some conditions.

*Research questions:*

+ What is the quality of textual responses in the narrative fields?

+ Is there any relationship between the word counts of response, question and
description fields?

+ What is the distribution between response word count and explanatory variables
such as the question, form topic, canton name, partner name, etc.

*Assumptions:*

+ Responses with a *larger word count* have more quality than the responses
with *smaller word count*.

In other words, we assume that *the more word the better is*. The limitations
are based on the unequal distribution of the data. The word count of responses
and questions can be related to other things, such as the questions require
short answers so then the responses tend to be shorter.

Additionally, we can have a cross-analysis to test these outcomes. It might be a
good idea to have a small subset of data anda sk an expert to test the
assumptions qualitatively. For instance, we can take the first twenty responses
with the highest word count and the last twenty responses with the lowest word
count. We chose the extreme directions because they point out the greatest
differences which are easier to test assumptions.

### Word count 

One issue with the nature of the questions is that they are only unique in a
form. These questions can be distributed across multiple forms. The questions
sharing the same name will have different meanings. For instance, the question
"*Cualitativo*" from the form "*Salud*" should imply different thing than the
question "*Cualitativo*" from the form "*Protección_VBG*".

In order to solve this kind of problem:

+ We can combine question with the form and also its folder label. There we can
achieve a unique name for each question.

+ Another thing to resolve this would be doing analysis to move the analysis up
to form level. In this file, we did both, therefore the analysis shown as below:

Count of responses per topic/question:
```{r count-response}
cp <- compact %>%
  mutate(.responseWordCount = word_count(response)) %>%
  mutate(.questionWordCount = word_count(question)) %>%
  group_by(labelForms) %>%
  select(
    labelForms, question, response, 
    .responseWordCount, .questionWordCount, 
    partnerName, canton,
    description, labelFormsRecode
  ) %>% 
  ungroup()
head(cp)
```

It's also a good practice to see the number of questions. For example, one
question has two responses, therefore they're short. Therefore, jittered points
are added to give a glance about the number of observations in the same plot.

Box plot form topics and response word counts (based on raw data):
```{r plot-topic-response-wc, paged.print=FALSE}
cp %>%
  gather(variable, value, .responseWordCount, .questionWordCount) %>%
  ggplot(aes(labelFormsRecode, value, fill = variable)) +
  geom_boxplot(outlier.colour = "orange", outlier.shape = 1, outlier.alpha = 0.35) +
  coord_flip() +
  geom_jitter(alpha = 0.35) +
  labs(
    title = "Response and Question word counts per form topic",
    subtitle = "A box plot distribution",
    caption = "*Please refer to recode table for label forms"
  ) +
  theme_ecuador1()
```

In the plot above, the outliers are shown in orange color. Outliers are the
points placed outside the whiskers, which is the long line, of the boxplot[^1].

The response word count distribution per form topic categorized by partner name:
```{r plot-partner-response-wc}
cp %>%
  gather(variable, value, .responseWordCount) %>% 
  ggplot(aes(labelFormsRecode, value)) +
  geom_boxplot(outlier.colour = "orange", outlier.shape = 1, outlier.alpha = 0.35) +
  coord_flip() +
  geom_jitter(alpha = 0.35) +
  facet_wrap(partnerName ~ ., scales = "free", ncol = 2) +
  labs(
    title = "Word count of responses by form topics per partner",
    subtitle = "A box plot distribution",
    caption = "*Please refer to recode table for label forms"
  ) +
  theme_ecuador1()
```

The response word count distribution per form topic categorized by canton name:
```{r plot-canton-response-wc}
cp %>%
  gather(variable, value, .responseWordCount) %>% 
  ggplot(aes(labelFormsRecode, value)) +
  geom_boxplot(outlier.colour = "orange", outlier.shape = 1, outlier.alpha = 0.35) +
  coord_flip() +
  geom_jitter(alpha = 0.35) +
  facet_grid(~canton, scales = "free") +
  labs(
    title = "Word count of responses by form topics per canton",
    subtitle = "A box plot distribution",
    caption = "*Please refer to recode table for label forms"
  ) +
  xlab(NULL) +
  ylab(NULL) +
  theme_ecuador1() +
  theme(panel.spacing = unit(0, "lines"))
```

*A caveat:* Reducing multiple values down to a single value should be avoided in
the early stages of the analysis because reducing hides a lot e.g. a bar chart
showing average the word count per partner. Some partners may write longer than
others, because:

1. They *actually* write longer than other partners.

2. The questions they answered require short answers.

### The "Description" field

Some questions have the description field giving extra details about the
questions.

Do some questions with the extra description field have better *response
quality* than the questions which does not have it?

Looking at the table containing form name, question, description and so on:

```{r, table1}
dsp <- cp %>% 
  mutate(.hasDescription = ifelse(is.na(description), FALSE, TRUE)) %>% 
  mutate(.descriptionWordCount = word_count(description)) %>%
  select(labelForms, labelFormsRecode, 
         .responseWordCount, .questionWordCount, .descriptionWordCount, 
         .hasDescription)
```

We see in the plot below that the response word counts per form and colored if a
response has a description field or not. Having a description field or not is
calculated as that a description field has a minimum one word.

The responses with the longest word
counts are the ones with description. Nevertheless, it is not so easy to see a
clear trend that there's a correlation between response word count and
description fields. Interestingly, the form *F15*, which is *Protección_VBG*, has
no description fields at all.

```{r, plot1}
dsp %>% 
  ggplot(aes(.responseWordCount, labelFormsRecode, color = .hasDescription)) +
  geom_point() +
  theme_ecuador1()
```

We look below the description word count and compare with the
response word count (and remove the categorical field displaying if the question
of response has a description field).

`TODO ANOVA`

### General trends {.tabset}

#### Median word count per partner

```{r}
cp %>% 
  group_by(partnerName) %>% 
  summarise(median = median(.responseWordCount)) %>% 
  ungroup() %>% 
  ggplot(aes(reorder(partnerName, -median), median)) +
  geom_bar(stat = "identity", width = 0.5, fill = "tomato2") + 
  labs(title = "Median word count of responses per partner",
       subtitle = sprintf("The total median rate %s", round(median(cp$.responseWordCount), 2))) +
  xlab("Partners") +
  ylab("Median value") +
  theme(axis.text.x = element_text(angle=65, vjust=0.6))
```

#### Median word count of responses

```{r}
med.wc <- compact %>% 
  mutate(.responseWordCount = word_count(response)) %>%
  mutate(Month = as.Date(paste0(Month, "-01"), format = "%Y-%m-%d")) %>%
  group_by(partnerName, Month) %>%
  summarize(median = median(.responseWordCount))

ggplot(med.wc, aes(Month, median, color = partnerName)) +
  geom_line() +
  geom_point() +
  labs(title = "Median word count of responses per partner over months",
       subtitle = sprintf("The average median rate %s", round(mean(med.wc$median), 2))) +
  xlab("Month") +
  ylab("Median word count") +
  scale_x_date(labels = date_format("%Y-%m")) +
  scale_color_brewer(palette = "Paired") +
  theme_ecuador1() +
  theme(legend.position = "bottom") +
  theme(legend.title = element_blank())
```

### The regression line

We can look at multiple continuous variables in our data.

- word count of response field: the dependent variable.

- word count of question field: an independent variable.

- word count of description field: an independent variable.

Scatter plots help understand the characteristics of those variables.
However, we miss a general understanding that is the *trend line*.

```{r, plot-linear-regression}
p1 <- dsp %>% 
  ggplot(aes(x = .responseWordCount, y = .questionWordCount)) +
  geom_jitter(alpha = 0.35) +
  geom_smooth(aes(colour = "linear"), method = "lm") +
  theme_ecuador1() +
  theme(legend.position = "none")
p2 <- dsp %>% 
  ggplot(aes(x = .responseWordCount, y = .descriptionWordCount)) +
  geom_jitter(alpha = 0.35) +
  geom_smooth(aes(colour = "linear"), method = "lm") +
  theme_ecuador1() +
  theme(legend.position = "none")
gridExtra::grid.arrange(p1, p2, ncol = 2)
```

The gray area around the lines shows the confidence band at the 0.95 level.
Although there's a straight slope in the linear regression line, we cannot
say that the trend line is robust because the confidence band representing the
uncertainty in the estimate is wide.

### Logistic regression

`TODO`

## Text analysis

In that section, we take text as data.

### Textual data preparation

Describe how to prepare textual data and what common steps are usually performed.

They are usually four steps involved in this process:

**1. Tokenization**

Tokenization means to split a text into tokens considered
meaningful units of text. A token can either be a word (and often it is) or a
group of words (such as *bigram*), or even a sentence that depends on the level
of analysis.

```{r}
tokens <- compact %>% 
  as_tibble() %>% 
  unnest_tokens(word, response)
kable_truncate(head(tokens), names(tokens)[seq(9)], trunc.level = 5)
```

Perform stemming, which you bring nouns/verbs back to infinitive forms, and
tokenization, which is separating words in meaningful pieces.

**2. Strip punctuation**

Punctuation is often not required in text analysis (unless a researcher wants to
tokenize the text based on a specific classifier *such as sentence tokens*);
therefore, they create noise.

**3. Convert text into lowercase**

When the text turned into lowercase, for instance, the words *respuesta* and
*Respuesta* will no longer be taken as different words.

**4. Exclude stopwords & numbers**

Stop words usually mean the most common words in a language that will bring no
significant results in analysis. They are overly distributed in the text and
they will not give so meaningful results itself. Stop-words are including
articles (*el/la*), conjunctions (*y*), pronouns (*yo/tú/etc.*) and so on.

In text mining, this process is usually done
after the text converted into lowercase so one does not have to provide stop
words including both lower and sentence case versions.

We import a list of Spanish stopwords data (source
[**here**](https://github.com/stopwords-iso/stopwords-es)) and perform a
*filtering join* returning tokens from textual data by excluding the words
listed in the stopwords.
that only returns the tokens not listed in the stopwords.

```{r}
es_stopwords <- get_es_stopwords()
tokens_sw <- tokens %>% 
  anti_join(es_stopwords, by = "word")
```

It's also possible to add more custom words such as *ACNUR* or *HIAS* if such
organization names are not desired in the results.

The original tokens had *`r nrow(tokens)`* rows but after stop words, it
decreased to *`r nrow(tokens_sw)`* and that
the change in between is *`r round(nrow(tokens_sw)*100/nrow(tokens))`*%.

### The most common words

The most common words in all responses:

```{r}
tokens_sw %>% 
  count(word, sort = TRUE) %>% 
  top_n(20, n) %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(word, n)) +
  geom_col() +
  coord_flip() +
  theme_ecuador1()
```

```{r, echo=FALSE, results='asis'}
recode.names <- unique(compact$labelFormsRecode)
body <- glue::glue(
  '
top.n <- 10L
tokens_sw %>%
  filter(labelFormsRecode == "{label.form.code}") %>%
  count(word) %>%
  arrange(desc(n)) %>% 
  slice(seq_len(top.n)) %>% 
  ggplot(aes(word, n)) +
  geom_col() +
  labs(
    title = "{label.form.code}",
    subtitle = sprintf("Top %d words", top.n),
    caption = "*Please refer to recode table for label forms"
  ) +
  coord_flip() +
  theme_ecuador1()
',
  label.form.code = recode.names
)
create_tabset(main = "### The most common words per topic", tabs = recode.names, body = body)
```

### Sentiment analysis

Sentiment analysis (also called as opinion mining) is a technique to understand
the emotional meanings of text given by a dictionary describing the
positive/negative words that already done by humans.

The responses seem to be written with a formal tone of voice; therefore, the
responses may not show any sentiment at all.

First, we find a sentiment lexicon for the Spanish language 
(source [here](https://sites.google.com/site/datascienceslab/projects/multilingualsentiment)).

```{r}
sentiments <- get_es_sentiments()
```

A wordcloud showing positive and negative words:
```{r}
tokens_sw %>%
  inner_join(sentiments, by = "word") %>%
  count(word, sentiment, sort = TRUE) %>%
  reshape2::acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  wordcloud::comparison.cloud(
    colors = c("#FF6961", "#228FCF"),
    max.words = 100,
    title.colors = "#5c5c5c"
  )
```


# References

```{r, echo=FALSE, results='asis'}
bibtex::read.bib("references.bib")
```

