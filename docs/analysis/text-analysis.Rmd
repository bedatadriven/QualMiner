
## 4.1. Textual data preparation

Describe how to prepare textual data and what common steps are usually performed.

They are usually four steps involved in this process:

**1. Tokenization**

Tokenization means to split a text into tokens considered
meaningful units of text. A token can either be a word (and often it is) or a
group of words (such as *bigram*), or even a sentence that depends on the level
of analysis.

```{r}
response.tokens_orig <- narratives %>%
  dplyr::filter(!is.na(response)) %>% ## remove NAs
  unnest_tokens(word, response, token = "words", to_lower = TRUE)

question.tokens_orig <- narratives %>% 
  dplyr::filter(!is.na(response)) %>% ## remove NAs
  unnest_tokens(word, question, token = "words", to_lower = TRUE)
```

Perform stemming, which you bring words (nouns/verbs) back to base or infinitive
forms, will be the next step after tokenization, so we can get the essence of
words.

**2. Strip punctuation**

Punctuation is often not required in text analysis (unless a researcher wants to
tokenize the text based on a specific classifier *such as sentence tokens*);
therefore, they create noise.

**3. Convert text into lowercase**

When the text turned into lowercase, for instance, the words *respuesta* and
*Respuesta* will no longer be taken as different words.

**4. Exclude stopwords & numbers**

Stop words usually mean the most common words in a language that will bring no
significant results in analysis. They are overly distributed in the text and
they will not give so meaningful results itself. Stop-words are including
articles (*el/la*), conjunctions (*y*), pronouns (*yo/tú/etc.*) and so on.

In text mining, this process is usually done
after the text converted into lowercase so one does not have to provide stop
words including both lower and sentence case versions.

We have imported a list of Spanish stopwords data (source
[**here**](https://github.com/stopwords-iso/stopwords-es), and that's the
alternative for `stopwords_es` list from the
[corpus](https://cran.r-project.org/package=corpus) package) and perform a
*filtering join* returning tokens from textual data by excluding the words
listed in the stopwords. that only returns the tokens not listed in the
stopwords.

```{r}
es_stopwords <- get_es_stopwords()

response.tokens_stopw <- response.tokens_orig %>% 
  anti_join(es_stopwords, by = "word")

act_codes <- data.frame(word = unique(question.tokens_orig$word[grep("act\\_\\d+.*", question.tokens_orig$word)]), stringsAsFactors = FALSE)

question.tokens_stopw <- question.tokens_orig %>% 
  anti_join(es_stopwords, by = "word") %>% 
  anti_join(act_codes, by = "word")
```

The original tokens for the response originally have 
*`r nrow(response.tokens_orig)`*
rows. However, after merging stop words, the number of rows have decreased to
*`r nrow(response.tokens_stopw)`* and that the change in between is
*`r round(convolve(nrow(response.tokens_stopw), 100)/nrow(response.tokens_orig))`*%.

It's also possible to add more custom words e.g. *ACNUR*, if some organization
names are not desired, or *violencia*, if some words are overused and brings no
further explanation, in the results.

**5. Perform stemming**

Stemming is a process that removes the suffixes (and sometimes prefixes) of the
words and bring them to the base form. We use "Hunspell" stemmer from the
package [**hunspell**](https://cran.r-project.org/package=hunspell) that
provides more precise stemming behavior.

From that point onwards, we will use stemmed words instead of the raw tokenized
words because stemmed words give us better information.

```{r}
response.tokens_stemmed <- response.tokens_stopw %>% 
  mutate(word_stem = corpus::text_tokens(word, stemmer = stem_hunspell)) %>% 
  unnest(word_stem)

question.tokens_stemmed <- question.tokens_stopw %>% 
  mutate(word_stem = corpus::text_tokens(word, stemmer = stem_hunspell)) %>% 
  unnest(word_stem)
```

After stemming, the words look like this:
```{r}
rbind(head(response.tokens_stemmed[c("word", "word_stem")], 5), "...") %>% 
  gt() %>% 
  gt_condensed_style() %>% 
  cols_align(
    align = "center",
    columns = c("word", "word_stem")
  )
```

<br>

<hr>

```{r INTERNAL-prepared-data-for-further-analysis}
response.tokens <- response.tokens_stemmed
question.tokens <- question.tokens_stemmed
```

## 4.2. Trends

```{r}
plot_most_common_words <- function(data, variable, fill.color, 
                                   which = c("responses", "questions")) {
  data %>%
    count({{variable}}, sort = TRUE) %>% 
    top_n(20, n) %>% 
    mutate(WORDS = reorder({{variable}}, n)) %>% 
    ggplot(aes(WORDS, n)) +
    geom_col(color = "black", fill = fill.color) +
    coord_flip() +
    theme_ecuador1() +
    labs(
      title = paste0("What are the most frequent words in all ", which, "?")
    ) +
    xlab(NULL) +
    ylab("Number of words")
}
```

### The most common words in all responses

```{r}
plot_most_common_words(response.tokens, word_stem, theme_color_codes("UNHCR blue"), "responses")
```

### The most common words in all questions

```{r}
plot_most_common_words(question.tokens, word_stem, theme_color_codes("Taffy"), "questions")
```

### The most common words in all responses per form topic

```{r, fig.width=10, fig.height=10}
## Reordering words separated into facets. Source:
## https://juliasilge.com/blog/reorder-within/
response.tokens %>%
  select(formNameRecode, word_stem) %>% 
  group_by(formNameRecode) %>% 
  count(word_stem) %>% 
  arrange(desc(n)) %>% 
  slice(seq(10)) %>% 
  ungroup() %>% 
  mutate(formNameRecode = forcats::as_factor(formNameRecode),
         Word = tidytext::reorder_within(word_stem, n, formNameRecode)) %>% 
  ggplot(aes(Word, n, fill = formNameRecode)) +
  geom_col(color = "black", show.legend = FALSE) +
  facet_wrap(~formNameRecode, scales = "free", ncol = 4) +
  coord_flip() +
  tidytext::scale_x_reordered() + 
  theme_ecuador1() +
  labs(
    title = "What are the most frequent words from responses in the form topics?",
    subtitle = "The relative frequency",
    caption = "*Please refer to recode table for label forms"
  ) +
  xlab(NULL) +
  ylab(NULL)
```

## 4.3. The key themes (by using term-frequency and tf-idf)

```{r, bind_tf_idf}
tf_idf_by_form <- response.tokens %>% 
  group_by(formNameRecode) %>% 
  count(word_stem) %>% 
  tidytext::bind_tf_idf(word_stem, formNameRecode, n)
```

We can look at the key themes in each topic by measuring the
term-frequency and tf-idf, short for term frequency–inverse document frequency.

```{r, fig.width=7, fig.height=7}
TOP.N <- 15
response.tokens %>% 
  group_by(Month) %>% 
  count(word_stem) %>% 
  arrange(desc(n)) %>% 
  slice(seq(TOP.N)) %>% 
  ungroup() %>% 
  mutate(Month = forcats::as_factor(Month),
         Word = tidytext::reorder_within(word_stem, n, Month)) %>% 
  ggplot(aes(Word, n, fill = Month)) +
  geom_col(stat = "identity") +
  coord_flip() +
  facet_wrap(~Month, scales = "free") +
  tidytext::scale_x_reordered() + 
  theme_ecuador1(border = TRUE) +
  theme(legend.position = "none") +
  labs(
    title = paste("The top", TOP.N, "most common words in responses per month"),
    subtitle = "Measured by using 'term frequency'"
  ) +
  xlab(NULL) +
  ylab(NULL)
```

Well, some words appear at the top at
almost all periods such as `personar`, `mujer`, `hombre`, `formación`,
`atención` and so on. 

```{r, fig.width=12, fig.height=10}
TOP.N <- 10
tf_idf_by_form %>% 
  arrange(desc(n)) %>% 
  slice(seq(TOP.N)) %>% 
  ungroup() %>% 
  mutate(formNameRecode = forcats::as_factor(formNameRecode),
         Word = tidytext::reorder_within(word_stem, tf_idf, formNameRecode)) %>% 
  ggplot(aes(Word, tf_idf, fill = formNameRecode)) +
  geom_col(stat = "identity") +
  coord_flip() +
  facet_wrap(~formNameRecode, scales = "free", ncol = 4) +
  tidytext::scale_x_reordered() + 
  theme_ecuador1(border = TRUE) +
  theme(legend.position = "none") +
  labs(
    title = paste("The top", TOP.N, "most common words in responses per month"),
    subtitle = "Measured by using 'tf-idf'"
  ) +
  xlab(NULL) +
  ylab(NULL)
```


We can closely look at some words to see how the term-frequency changes over
time.
For example, we can focus on **VBG**, short for
*`r recode_tbl[recode_tbl$formNameRecode == "VBG","formName",drop=TRUE]`*
placed in the folder
*`r recode_tbl[recode_tbl$formNameRecode == "VBG","folderName",drop=TRUE]`*.

```{r, fig.width=6, fig.height=10}
SELECTED.FORM <- "VBG"
tf_first_per_month <- response.tokens %>% 
  group_by(formNameRecode, Month) %>% 
  count(word_stem) %>%
  dplyr::filter(formNameRecode == SELECTED.FORM) %>% 
  tidytext::bind_tf_idf(word_stem, Month, n) %>% 
  dplyr::filter(!word_stem %in% c("vbg", "unfpa", "hias")) %>% ## filter out some words that can be redundant
  ungroup() %>% 
  select(Month, word_stem, tf) %>% 
  arrange(desc(tf)) %>%
  group_by(Month) %>% 
  slice(seq(3)) %>% 
  ungroup()

tf_first_per_month_ts <-
  tf_first_per_month %>%
  mutate(Month = yearmonth(Month)) %>% 
  as_tsibble(index = "Month", key = "word_stem") %>% 
  fill_gaps(tf = 0, .full = TRUE)

tf_first_per_month_ts %>% 
  ggplot(aes(Month, tf, col = word_stem, group = word_stem)) +
  geom_point() +
  geom_line() +
  facet_grid(rows = vars(word_stem), scales = "free_y") +
  scale_x_date(labels = scales::date_format("%Y-%m")) +
  labs(
    title = paste0("Word frequency over time in '", SELECTED.FORM, "' form")
  )+
  theme_ecuador1(border=TRUE, panel_spacing = 0.25) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1.1)) +
  theme(legend.position = "none")
```


## 4.4. The relationship between words (n-grams)

### Higher order n-grams

We can search the multi-type terms in the responses.

```{r}
response.tokens.bigrams <- narratives %>%
  dplyr::filter(!is.na(response)) %>%
  unnest_tokens(word, response, token = "ngrams", n = 2, to_lower = TRUE) %>% 
  tidyr::separate(word, c("word1", "word2"), sep = " ") %>% 
  ## filter stopwords
  dplyr::filter(
    !word1 %in% es_stopwords$word,
    !word2 %in% es_stopwords$word
  ) %>% 
  ## stem words
  mutate(word1_stem = corpus::text_tokens(word1, stemmer = stem_hunspell)) %>% 
  mutate(word2_stem = corpus::text_tokens(word2, stemmer = stem_hunspell)) %>% 
  unnest(word1_stem) %>% 
  unnest(word2_stem) %>% 
  unite(bigram, word1_stem, word2_stem, sep = " ")
```

So the end result looks like this:
```{r}
rbind(head(response.tokens.bigrams[c("word1", "word2", "bigram")], 5), "...") %>% 
  gt() %>% 
  gt_condensed_style() %>% 
  cols_align(
    align = "center",
    columns = c("word1", "word2", "bigram")
  )
```

*tf-idf* values can also be calculated for bigrams, and visualized within each
reporting/implementing partner, province/canton and so forth.

```{r}
response.tokens.bigrams.tf_idf <- 
  response.tokens.bigrams %>% 
  count(formNameRecode, bigram) %>%
  bind_tf_idf(bigram, formNameRecode, n) %>%
  arrange(desc(tf_idf))
```

```{r,fig.height=15, fig.width=10}
response.tokens.bigrams.tf_idf %>%
  group_by(formNameRecode) %>%
  arrange(desc(tf_idf)) %>%
  slice(seq(12)) %>% 
  ungroup() %>%
  mutate(bigram = reorder(bigram, tf_idf)) %>%
  ggplot(aes(bigram, tf_idf, fill = formNameRecode)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ formNameRecode, ncol = 2, scales = "free") +
  theme_ecuador1(border = TRUE) +
  coord_flip() +
  labs(title = "tf-idf of bigram within form topics") +
  xlab(NULL) +
  ylab(NULL)
```

## 4.4. Term-frequency matrix

`TODO`

## 4.5. The response structure and the sentence lengths

`TODO`


## 4.x. Sentiment analysis

Sentiment analysis (also called as opinion mining) is a technique to understand
the emotional meanings of text given by a dictionary describing the
positive/negative words that already done by humans.

The responses seem to be written with a formal tone of voice; therefore, the
responses may not show any sentiment at all.

First, we find a sentiment lexicon for the Spanish language 
(source [here](https://sites.google.com/site/datascienceslab/projects/multilingualsentiment)).

```{r}
sentiments <- get_es_sentiments()
```

A wordcloud showing positive and negative words in the responses:
```{r, warning=FALSE}
response.tokens %>%
  inner_join(sentiments, by = c("word_stem" = "word")) %>%
  count(word, sentiment, sort = TRUE) %>%
  reshape2::acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  wordcloud::comparison.cloud(
    colors = c("#FF6961", "#228FCF"),
    max.words = 100,
    title.colors = "#5c5c5c"
  )
```
