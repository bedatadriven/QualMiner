
### Textual data preparation

Describe how to prepare textual data and what common steps are usually performed.

They are usually four steps involved in this process:

**1. Tokenization**

Tokenization means to split a text into tokens considered
meaningful units of text. A token can either be a word (and often it is) or a
group of words (such as *bigram*), or even a sentence that depends on the level
of analysis.

```{r}
response.tokens_orig <- narratives %>%
  dplyr::filter(!is.na(response)) %>% ## remove NAs
  unnest_tokens(word, response, to_lower = TRUE)

question.tokens_orig <- narratives %>% 
  dplyr::filter(!is.na(response)) %>% ## remove NAs
  unnest_tokens(word, question, to_lower = TRUE)
```

Perform stemming, which you bring nouns/verbs back to infinitive forms, might be
the next step after tokenization, which only separates words in essence.
`TODO stemming`

**2. Strip punctuation**

Punctuation is often not required in text analysis (unless a researcher wants to
tokenize the text based on a specific classifier *such as sentence tokens*);
therefore, they create noise.

**3. Convert text into lowercase**

When the text turned into lowercase, for instance, the words *respuesta* and
*Respuesta* will no longer be taken as different words.

**4. Exclude stopwords & numbers**

Stop words usually mean the most common words in a language that will bring no
significant results in analysis. They are overly distributed in the text and
they will not give so meaningful results itself. Stop-words are including
articles (*el/la*), conjunctions (*y*), pronouns (*yo/t√∫/etc.*) and so on.

In text mining, this process is usually done
after the text converted into lowercase so one does not have to provide stop
words including both lower and sentence case versions.

We have imported a list of Spanish stopwords data (source
[**here**](https://github.com/stopwords-iso/stopwords-es)) and perform a
*filtering join* returning tokens from textual data by excluding the words
listed in the stopwords.
that only returns the tokens not listed in the stopwords.

```{r}
es_stopwords <- get_es_stopwords()

response.tokens <- response.tokens_orig %>% 
  anti_join(es_stopwords, by = "word")

act_codes <- data.frame(word = unique(question.tokens_orig$word[grep("act\\_\\d+.*", question.tokens_orig$word)]), stringsAsFactors = FALSE)

question.tokens <- question.tokens_orig %>% 
  anti_join(es_stopwords, by = "word") %>% 
  anti_join(act_codes, by = "word")
```

It's also possible to add more custom words e.g. *ACNUR*, if some organization
names are not desired, or *violencia*, if some words are overused and brings no
further explanation, in the results.

The original tokens for the response originally have *`r nrow(response.tokens_orig)`*
rows. However, after merging stop words, the number of rows have decreased to
*`r nrow(response.tokens)`* and that the change in between is
*`r round(convolve(nrow(response.tokens), 100)/nrow(response.tokens_orig))`*%.

### The most common words

```{r}
plot_most_common_words <- function(data, fill.color, which = c("responses", "questions")) {
  data %>%
    count(word, sort = TRUE) %>% 
    top_n(20, n) %>% 
    mutate(word = reorder(word, n)) %>% 
    ggplot(aes(word, n)) +
    geom_col(color = "black", fill = {{ fill.color }}) +
    coord_flip() +
    theme_ecuador1() +
    labs(
      title = paste0("What are the most frequent words in all ", which, "?")
    ) +
    xlab(NULL) +
    ylab("Number of words")
}
```

```{r}
plot_most_common_words(response.tokens, theme_color_codes("UNHCR blue"), "responses")
```

```{r}
plot_most_common_words(question.tokens, theme_color_codes("Taffy"), "questions")
```

```{r, fig.width=10, fig.height=10}
## Reordering words separated into facets. Source:
## https://juliasilge.com/blog/reorder-within/
response.tokens %>%
  select(formNameRecode, word) %>% 
  group_by(formNameRecode) %>% 
  count(word) %>% 
  arrange(desc(n)) %>% 
  slice(seq(10)) %>% 
  ungroup() %>% 
  mutate(formNameRecode = forcats::as_factor(formNameRecode),
         word = tidytext::reorder_within(word, n, formNameRecode)) %>% 
  ggplot(aes(word, n, fill = formNameRecode)) +
  geom_col(color = "black", show.legend = FALSE) +
  facet_wrap(~formNameRecode, scales = "free", ncol = 4) +
  coord_flip() +
  tidytext::scale_x_reordered() + 
  theme_ecuador1() +
  labs(
    title = "What are the most frequent words from responses in the form topics?",
    subtitle = "The relative frequency",
    caption = "*Please refer to recode table for label forms"
  ) +
  xlab(NULL) +
  ylab(NULL)
```

### Sentiment analysis

Sentiment analysis (also called as opinion mining) is a technique to understand
the emotional meanings of text given by a dictionary describing the
positive/negative words that already done by humans.

The responses seem to be written with a formal tone of voice; therefore, the
responses may not show any sentiment at all.

First, we find a sentiment lexicon for the Spanish language 
(source [here](https://sites.google.com/site/datascienceslab/projects/multilingualsentiment)).

```{r}
sentiments <- get_es_sentiments()
```

A wordcloud showing positive and negative words in the responses:
```{r}
response.tokens_sw %>%
  inner_join(sentiments, by = "word") %>%
  count(word, sentiment, sort = TRUE) %>%
  reshape2::acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  wordcloud::comparison.cloud(
    colors = c("#FF6961", "#228FCF"),
    max.words = 100,
    title.colors = "#5c5c5c"
  )
```

