
### Textual data preparation

Describe how to prepare textual data and what common steps are usually performed.

They are usually four steps involved in this process:

**1. Tokenization**

Tokenization means to split a text into tokens considered
meaningful units of text. A token can either be a word (and often it is) or a
group of words (such as *bigram*), or even a sentence that depends on the level
of analysis.

```{r}
tokens <- narratives %>% 
  as_tibble() %>% 
  unnest_tokens(word, response)
head(tokens) %>% 
  gt()
```

Perform stemming, which you bring nouns/verbs back to infinitive forms, and
tokenization, which is separating words in meaningful pieces.

**2. Strip punctuation**

Punctuation is often not required in text analysis (unless a researcher wants to
tokenize the text based on a specific classifier *such as sentence tokens*);
therefore, they create noise.

**3. Convert text into lowercase**

When the text turned into lowercase, for instance, the words *respuesta* and
*Respuesta* will no longer be taken as different words.

**4. Exclude stopwords & numbers**

Stop words usually mean the most common words in a language that will bring no
significant results in analysis. They are overly distributed in the text and
they will not give so meaningful results itself. Stop-words are including
articles (*el/la*), conjunctions (*y*), pronouns (*yo/t√∫/etc.*) and so on.

In text mining, this process is usually done
after the text converted into lowercase so one does not have to provide stop
words including both lower and sentence case versions.

We import a list of Spanish stopwords data (source
[**here**](https://github.com/stopwords-iso/stopwords-es)) and perform a
*filtering join* returning tokens from textual data by excluding the words
listed in the stopwords.
that only returns the tokens not listed in the stopwords.

```{r}
es_stopwords <- get_es_stopwords()
tokens_sw <- tokens %>% 
  anti_join(es_stopwords, by = "word")
```

It's also possible to add more custom words such as *ACNUR* or *HIAS* if such
organization names are not desired in the results.

The original tokens had *`r nrow(tokens)`* rows but after stop words, it
decreased to *`r nrow(tokens_sw)`* and that
the change in between is *`r round(nrow(tokens_sw)*100/nrow(tokens))`*%.

### The most common words

The most common words in all responses:

```{r}
tokens_sw %>% 
  count(word, sort = TRUE) %>% 
  top_n(20, n) %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(word, n)) +
  geom_col() +
  coord_flip() +
  theme_ecuador1()
```

```{r, echo=FALSE, results='asis'}
recode.names <- unique(narratives$labelFormsRecode)
body <- glue::glue(
  '
top.n <- 10L
tokens_sw %>%
  dplyr::filter(labelFormsRecode == "{label.form.code}") %>%
  count(word) %>%
  arrange(desc(n)) %>% 
  slice(seq_len(top.n)) %>% 
  ggplot(aes(word, n)) +
  geom_col() +
  labs(
    title = "{label.form.code}",
    subtitle = sprintf("Top %d words", top.n),
    caption = "*Please refer to recode table for label forms"
  ) +
  coord_flip() +
  theme_ecuador1()
',
  label.form.code = recode.names
)
create_tabset(main = "### The most common words per topic", tabs = recode.names, body = body)
```

### Sentiment analysis

Sentiment analysis (also called as opinion mining) is a technique to understand
the emotional meanings of text given by a dictionary describing the
positive/negative words that already done by humans.

The responses seem to be written with a formal tone of voice; therefore, the
responses may not show any sentiment at all.

First, we find a sentiment lexicon for the Spanish language 
(source [here](https://sites.google.com/site/datascienceslab/projects/multilingualsentiment)).

```{r}
sentiments <- get_es_sentiments()
```

A wordcloud showing positive and negative words:
```{r}
tokens_sw %>%
  inner_join(sentiments, by = "word") %>%
  count(word, sentiment, sort = TRUE) %>%
  reshape2::acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  wordcloud::comparison.cloud(
    colors = c("#FF6961", "#228FCF"),
    max.words = 100,
    title.colors = "#5c5c5c"
  )
```

