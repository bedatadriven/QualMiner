---
title: "Text analysis"
author: "Metin Yazici, BeDataDriven"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r header,include=FALSE}
source(file.path("..", "R", "global-header.R"))
```

```{r code-setup, include=FALSE}
## Libraries
library(tidyverse)
library(ggplot2)
library(tidytext)
## functions
source(file.path("..", "R", "analysis-methods.R"))
## Pre-set the bw theme for ggplot (for plots don't use any custom theme).
ggplot2::theme_set(ggplot2::theme_bw())
## knitr options
knitr::opts_chunk$set(echo = TRUE, paged.print = FALSE)
```

## Introduction

In that section, we take text as data.

```{r data-load}
compact <- readRDS(file.path("..", "data", "compact.RDS"))
compact <- inner_join(recode_tbl, compact, by = "labelForms")
```

## Textual data processing

Describe how to process textual data and what common steps are usually performed.

### Tokenization

Tokenization means to split text into tokens considered
meaningful units of text. A token can either be a word (and often it is) or a
group of words (such as *bigram*), or even a sentence that depends on the level
of analysis.

```{r}
tokens <- compact %>% 
  as_tibble() %>% 
  unnest_tokens(word, response)
head(tokens)
```

Perform stemming, which you bring nouns/verbs back to infinitive forms, and
tokenization, which is separating words in meaningful pieces.

There are usually a list of such words for each language and they are called as
stop-words as they are overly distributed in the text and they will not give so
meaningful results itself. Stop-words are including articles (*el/la*),
conjunctions (*y*), pronouns (*yo/t√∫/etc.*) and so on. We explain below how to
remove them.

### Strip punctuation

Punctuation is often not required in text analysis (unless a researcher wants to
tokenize the text based on a specific classifier *such as sentence tokens*);
therefore, they create noise.

### Convert text into lowercase

When the text turned into lowercase, for instance, the words *respuesta* and
*Respuesta* will no longer be taken as different words.

### Exclude stopwords & numbers

Stop words usually mean the most common words in a language that will bring no
significant results in an analysis. In text mining, this process is usually done
after the text converted into lowercase so one does not have to provide stop
words including both lower and sentence case versions.

We import a list of Spanish stopwords data (source
[**here**](https://github.com/stopwords-iso/stopwords-es)) and perform a
*filtering join* returning tokens from textual data by excluding the words
listed in the stopwords.
that only returns the tokens not listed in the stopwords.

```{r}
es_stopwords <- get_es_stopwords()
tokens_sw <- tokens %>% 
  anti_join(es_stopwords, by = "word")
head(tokens_sw)
```

It's also possible to add more custom words such as *ACNUR* or *HIAS* if such organization names are not desired in the results.

The original tokens had *`r nrow(tokens)`* rows but after stop words, it
decreased to *`r nrow(tokens_sw)`* and that
the change in between is *`r round(nrow(tokens_sw)*100/nrow(tokens))`*%.

## Analysis

### The most common words

The most common words in all responses:

```{r}
tokens_sw %>% 
  count(word, sort = TRUE) %>% 
  top_n(20) %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(word, n)) +
  geom_col() +
  coord_flip() +
  theme_ecuador1()
```

```{r, paged.print=FALSE}
top.n <- 10L
tokens_sw %>% 
  count(labelFormsRecode, word) %>%
  arrange(desc(n)) %>% 
  group_by(labelFormsRecode) %>% 
  slice(seq_len(top.n)) %>% 
  ungroup() %>% 
  ggplot(aes(word, n)) +
  geom_col() +
  facet_wrap(~ labelFormsRecode, scales = "free") +
  labs(
    title = "The most common words per topic label",
    subtitle = sprintf("Top %d words", top.n),
    caption = "*Please refer to recode table for label forms"
  ) +
  xlab(NULL) +
  coord_flip() +
  theme_ecuador1()
```

### Word frequencies

The word frequencies per topic
